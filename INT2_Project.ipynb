{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymanish/INT2-ImageNet-Pytorch-Model/blob/main/INT2_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GRgKEwh2fXQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In order to use torchVision you must run the command\n",
        "'tensorboard --logdir=runs'\n",
        "From terminal in the same directory as the python file\n",
        "\n",
        "Then open http://localhost:6006\n",
        "\n",
        "Install PyTorch at https://pytorch.org/get-started/locally/\n",
        "\"\"\"\n",
        "# using the time you can tell the program to run for an amount of time instead of a specific number of epochs\n",
        "from timeit import default_timer as timer\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MBl7bon2t01"
      },
      "outputs": [],
      "source": [
        "\n",
        "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# obtains the official training and test sets for the CIFAR10 Dataset\n",
        "#trainset = torchvision.datasets.CIFAR10(\n",
        "#    root='./data',\n",
        "#    train=True,\n",
        "#    download=True,\n",
        "#    transform=transform\n",
        "#)\n",
        "#testset = torchvision.datasets.CIFAR10(\n",
        "#    root='./data',\n",
        "#    train=False,\n",
        "#    download=True,\n",
        "#    transform=transform\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426a4dd2-a757-4875-f181-b70121933080",
        "id": "N_kd49RxPg-R"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 35486229.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "#RANDOM IMAGE AUGMENTATION TO THE TRAINING IMAGES\n",
        "\n",
        "random_transform = transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# obtains the official training and test sets for the CIFAR10 Dataset\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=random_transform\n",
        ")\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIlc90ma4Dl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e10936-532c-43ef-e5ce-e614cf8b5e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W] : torch.Size([50, 3, 32, 32])\n",
            "Shape of y: torch.Size([50]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "#how many values are tested in each iteration\n",
        "batch_size = 50\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1) #numworkers = 1\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "for images, lables in testloader:\n",
        "  print(f\"Shape of X [N, C, H, W] : {images.shape}\")\n",
        "  print(f\"Shape of y: {lables.shape} {lables.dtype}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show images\n",
        "def show_batch(dl):\n",
        "    for images, labels in dl:\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(torchvision.utils.make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
        "        break"
      ],
      "metadata": {
        "id": "np9lmM1TTa3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_batch(trainloader)"
      ],
      "metadata": {
        "id": "ZBWh595KTgWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYjyVWht5_4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02f8cd0-b9a6-40f5-bf20-b372e6d817df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Get training device. This will be the CPU or GPU(cuda) depending on availability\\ndevice = torch.device(\\'cuda:0\\' if torch.cuda.is_available() else \\'cpu\\')\\nprint(f\"Using {device} device\")\\n\\n# Define model. In pytorch this inherits from nn.Module. The networks layers are defined in __init__. \\n# Forward is how we specify how data passes through the network. If the GPU is available we pass operations to that as it is faster.\\nclass NeuralNetwork(nn.Module):\\n     def __init__(self, num_classes=len(classes)):\\n         super(NeuralNetwork, self).__init__()\\n         self.conv = nn.Sequential(\\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),#change strides on some convolutions first as 1 and increase kernel size\\n            nn.ReLU(inplace=True),\\n            nn.MaxPool2d(kernel_size=2),\\n            nn.Conv2d(64, 192, kernel_size=3, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.MaxPool2d(kernel_size=2),\\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.MaxPool2d(kernel_size=2),\\n        )\\n\\n#         self.conv1 = nn.Conv2d(3, 6, 5)\\n#         self.pool = nn.MaxPool2d(2, 2)\\n#         self.conv2 = nn.Conv2d(6, 16, 5)\\n#         self.fc1 = nn.Linear(16 * 5 * 5,120)\\n#         self.fc2 = nn.Linear(120, 84)\\n#         self.fc3 = nn.Linear(84, 10)\\n\\n         self.avgpool = nn.AdaptiveAvgPool2d((6,6))\\n\\n         self.linear = nn.Sequential(\\n            nn.Dropout(),\\n            # if you set stride to equal 2 then change the fisrt 4096 to 255 * 2 * 2\\n            nn.Linear(4096, 4096),\\n            nn.ReLU(inplace=True),\\n            nn.Dropout(),\\n            nn.Linear(4096, 4096),\\n            nn.ReLU(inplace=True),\\n            nn.Linear(4096, num_classes),\\n        )\\n\\n     def forward(self, x):\\n        x = self.conv(x)\\n        # if you set stride to equal 2 then change 4096 to 255 * 2 * 2\\n        x = x.view(x.size(0), 4096)\\n        x = torch.flatten(x, 1)\\n        x = self.linear(x)\\n        return x\\n\\n\\nmodel = NeuralNetwork().to(device)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''\n",
        "# Get training device. This will be the CPU or GPU(cuda) depending on availability\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model. In pytorch this inherits from nn.Module. The networks layers are defined in __init__.\n",
        "# Forward is how we specify how data passes through the network. If the GPU is available we pass operations to that as it is faster.\n",
        "class NeuralNetwork(nn.Module):\n",
        "     def __init__(self, num_classes=len(classes)):\n",
        "         super(NeuralNetwork, self).__init__()\n",
        "         self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),#change strides on some convolutions first as 1 and increase kernel size\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         self.fc1 = nn.Linear(16 * 5 * 5,120)\n",
        "#         self.fc2 = nn.Linear(120, 84)\n",
        "#         self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "         self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
        "\n",
        "         self.linear = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            # if you set stride to equal 2 then change the fisrt 4096 to 255 * 2 * 2\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "     def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # if you set stride to equal 2 then change 4096 to 255 * 2 * 2\n",
        "        x = x.view(x.size(0), 4096)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398a455d-2469-4597-9ccc-e30da94b9951",
        "id": "NZTz-8aKWLLC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0 device\n"
          ]
        }
      ],
      "source": [
        "# Get training device. This will be the CPU or GPU(cuda) depending on availability\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model. In pytorch this inherits from nn.Module. The networks layers are defined in __init__.\n",
        "# Forward is how we specify how data passes through the network. If the GPU is available we pass operations to that as it is faster.\n",
        "class NeuralNetwork(nn.Module):\n",
        "     def __init__(self, num_classes=len(classes)):\n",
        "         super(NeuralNetwork, self).__init__()\n",
        "         self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),#change strides on some convolutions first as 1 and increase kernel size\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         self.fc1 = nn.Linear(16 * 5 * 5,120)\n",
        "#         self.fc2 = nn.Linear(120, 84)\n",
        "#         self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "         self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
        "\n",
        "         self.linear = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            # if you set stride to equal 2 then change the fisrt 4096 to 255 * 2 * 2\n",
        "            nn.Linear(4096*2, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "     def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # if you set stride to equal 2 then change 4096 to 255 * 2 * 2\n",
        "        x = x.view(x.size(0), 4096*2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = NeuralNetwork().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVZ-JyxR7W1C"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizerSGD = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "optimizerAdam = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "optimizerRMSprop = torch.optim.RMSprop(model.parameters(), lr =lr)\n",
        "optimizerAdaDelta = torch.optim.Adadelta(model.parameters(), lr =lr)\n",
        "optimizerAdaGrad = torch.optim.Adagrad(model.parameters(),lr = lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipUm8Wen7eOM"
      },
      "outputs": [],
      "source": [
        "def train(trainloader, model, loss_fn, optimizer):\n",
        "    size = len(trainloader.dataset)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    batch_loss = 0.0\n",
        "    batchTime = timer()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 500 == 499:\n",
        "            loss, current = loss.item(), i * len(inputs)\n",
        "            print(f'[{(i + 1) * batch_size:>5d}/{size:5d}] loss: {(running_loss - batch_loss) / 100:.4f} (batch time: {timer() - batchTime:>.4f})')\n",
        "            batchTime = timer()\n",
        "            batch_loss = running_loss\n",
        "\n",
        "    return running_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MA3BBL7-g7Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn0jy-vW7nGg"
      },
      "outputs": [],
      "source": [
        "def test(testloader, model, loss_fn):\n",
        "\n",
        "    size = len(testloader.dataset)\n",
        "    num_batches = len(testloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    class_probs = []\n",
        "    class_label = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            output = model(images)\n",
        "\n",
        "            test_loss += loss_fn(output, labels).item()\n",
        "            correct += (output.argmax(1) == labels).type(torch.float).sum().item()\n",
        "\n",
        "            class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
        "\n",
        "            class_probs.append(class_probs_batch)\n",
        "            class_label.append(labels)\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "\n",
        "    test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
        "    test_label = torch.cat(class_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VrRl8ihZgti"
      },
      "outputs": [],
      "source": [
        "dt_string = datetime.now().strftime(\"%Y%m%d_%H;%M;%S\")\n",
        "Path(f\"./models/{dt_string}/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "writer = SummaryWriter(f'./logs/{dt_string}')\n",
        "writer.add_graph(model, images.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMpd4MvN7uYR",
        "outputId": "9df72f8d-2613-4b2e-e444-ffb7b6cd82be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "[25000/50000] loss: 11.5121 (batch time: 22.0893)\n",
            "[50000/50000] loss: 11.5108 (batch time: 20.5440)\n",
            "Test Error: \n",
            "Accuracy: 15.6%, Avg loss: 2.301537\n",
            "Time elapsed: 47.5297 seconds\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "[25000/50000] loss: 11.5068 (batch time: 21.0538)\n",
            "[50000/50000] loss: 11.4987 (batch time: 22.1443)\n",
            "Time elapsed: 43.2217 seconds\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "[25000/50000] loss: 11.4626 (batch time: 21.1682)\n",
            "[50000/50000] loss: 11.0841 (batch time: 21.9559)\n",
            "Time elapsed: 43.1468 seconds\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "[25000/50000] loss: 10.0389 (batch time: 21.8672)\n",
            "[50000/50000] loss: 9.3411 (batch time: 21.3018)\n",
            "Time elapsed: 43.1925 seconds\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "[25000/50000] loss: 8.9438 (batch time: 22.4353)\n",
            "[50000/50000] loss: 8.7122 (batch time: 21.2279)\n",
            "Time elapsed: 43.6879 seconds\n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "[25000/50000] loss: 8.5216 (batch time: 22.4539)\n",
            "[50000/50000] loss: 8.2979 (batch time: 21.4037)\n",
            "Time elapsed: 43.8893 seconds\n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "[25000/50000] loss: 8.1186 (batch time: 21.8511)\n",
            "[50000/50000] loss: 8.0105 (batch time: 22.3717)\n",
            "Time elapsed: 44.2476 seconds\n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "[25000/50000] loss: 7.8064 (batch time: 21.0171)\n",
            "[50000/50000] loss: 7.6543 (batch time: 22.2387)\n",
            "Time elapsed: 43.28 seconds\n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "[25000/50000] loss: 7.3882 (batch time: 21.6823)\n",
            "[50000/50000] loss: 7.2666 (batch time: 21.6061)\n",
            "Time elapsed: 43.3135 seconds\n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "[25000/50000] loss: 7.0558 (batch time: 22.2945)\n",
            "[50000/50000] loss: 7.0072 (batch time: 20.9720)\n",
            "Time elapsed: 43.2902 seconds\n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "[25000/50000] loss: 6.8109 (batch time: 22.3246)\n",
            "[50000/50000] loss: 6.6608 (batch time: 22.1393)\n",
            "Test Error: \n",
            "Accuracy: 53.8%, Avg loss: 1.263799\n",
            "Time elapsed: 48.3096 seconds\n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "[25000/50000] loss: 6.5537 (batch time: 21.6533)\n",
            "[50000/50000] loss: 6.3832 (batch time: 21.7131)\n",
            "Time elapsed: 43.3902 seconds\n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "[25000/50000] loss: 6.3127 (batch time: 22.4455)\n",
            "[50000/50000] loss: 6.0889 (batch time: 21.0081)\n",
            "Time elapsed: 43.4779 seconds\n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "[25000/50000] loss: 5.9597 (batch time: 22.5299)\n",
            "[50000/50000] loss: 5.8977 (batch time: 22.1504)\n",
            "Time elapsed: 44.7041 seconds\n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "[25000/50000] loss: 5.7559 (batch time: 21.0672)\n",
            "[50000/50000] loss: 5.6207 (batch time: 22.2120)\n",
            "Time elapsed: 43.3042 seconds\n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "[25000/50000] loss: 5.5378 (batch time: 21.4510)\n",
            "[50000/50000] loss: 5.4068 (batch time: 21.9414)\n",
            "Time elapsed: 43.4154 seconds\n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "[25000/50000] loss: 5.3079 (batch time: 22.3488)\n",
            "[50000/50000] loss: 5.2517 (batch time: 21.0957)\n",
            "Time elapsed: 43.4668 seconds\n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "[25000/50000] loss: 5.0903 (batch time: 22.2719)\n",
            "[50000/50000] loss: 5.0539 (batch time: 21.8359)\n",
            "Time elapsed: 44.1523 seconds\n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "[25000/50000] loss: 5.0084 (batch time: 21.3046)\n",
            "[50000/50000] loss: 4.8469 (batch time: 22.1730)\n",
            "Time elapsed: 43.5015 seconds\n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "[25000/50000] loss: 4.7911 (batch time: 21.2650)\n",
            "[50000/50000] loss: 4.7454 (batch time: 22.0560)\n",
            "Time elapsed: 43.3501 seconds\n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "[25000/50000] loss: 4.5989 (batch time: 22.3702)\n",
            "[50000/50000] loss: 4.6068 (batch time: 20.9080)\n",
            "Test Error: \n",
            "Accuracy: 68.3%, Avg loss: 0.877415\n",
            "Time elapsed: 48.1717 seconds\n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "[25000/50000] loss: 4.4788 (batch time: 21.0836)\n",
            "[50000/50000] loss: 4.4700 (batch time: 22.0736)\n",
            "Time elapsed: 43.1817 seconds\n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "[25000/50000] loss: 4.3392 (batch time: 21.1972)\n",
            "[50000/50000] loss: 4.3334 (batch time: 21.9691)\n",
            "Time elapsed: 43.1919 seconds\n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "[25000/50000] loss: 4.2770 (batch time: 22.3267)\n",
            "[50000/50000] loss: 4.1277 (batch time: 20.9242)\n",
            "Time elapsed: 43.2775 seconds\n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "[25000/50000] loss: 4.0923 (batch time: 22.2977)\n",
            "[50000/50000] loss: 4.0730 (batch time: 22.3082)\n",
            "Time elapsed: 44.6433 seconds\n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.9806 (batch time: 21.2147)\n",
            "[50000/50000] loss: 4.0410 (batch time: 22.2625)\n",
            "Time elapsed: 43.5035 seconds\n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.8810 (batch time: 21.5602)\n",
            "[50000/50000] loss: 3.8722 (batch time: 22.1615)\n",
            "Time elapsed: 43.7458 seconds\n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.7376 (batch time: 22.5082)\n",
            "[50000/50000] loss: 3.7237 (batch time: 21.0986)\n",
            "Time elapsed: 43.6303 seconds\n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.6419 (batch time: 22.6280)\n",
            "[50000/50000] loss: 3.7290 (batch time: 22.2132)\n",
            "Time elapsed: 44.8767 seconds\n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.5533 (batch time: 21.5686)\n",
            "[50000/50000] loss: 3.5740 (batch time: 22.4130)\n",
            "Time elapsed: 44.0085 seconds\n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.4547 (batch time: 21.7067)\n",
            "[50000/50000] loss: 3.4772 (batch time: 22.1250)\n",
            "Test Error: \n",
            "Accuracy: 77.7%, Avg loss: 0.640303\n",
            "Time elapsed: 47.7665 seconds\n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.4039 (batch time: 22.4661)\n",
            "[50000/50000] loss: 3.4013 (batch time: 22.3717)\n",
            "Time elapsed: 44.8629 seconds\n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.3178 (batch time: 21.5339)\n",
            "[50000/50000] loss: 3.3417 (batch time: 22.4195)\n",
            "Time elapsed: 43.9782 seconds\n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.2479 (batch time: 22.3235)\n",
            "[50000/50000] loss: 3.2097 (batch time: 21.1910)\n",
            "Time elapsed: 43.5376 seconds\n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.1228 (batch time: 22.3774)\n",
            "[50000/50000] loss: 3.1504 (batch time: 22.1755)\n",
            "Time elapsed: 44.5889 seconds\n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.1315 (batch time: 21.3816)\n",
            "[50000/50000] loss: 3.0341 (batch time: 22.1855)\n",
            "Time elapsed: 43.5919 seconds\n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "[25000/50000] loss: 3.0254 (batch time: 21.7340)\n",
            "[50000/50000] loss: 3.0306 (batch time: 21.8009)\n",
            "Time elapsed: 43.5613 seconds\n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.9438 (batch time: 22.2546)\n",
            "[50000/50000] loss: 2.9500 (batch time: 21.0561)\n",
            "Time elapsed: 43.3362 seconds\n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.8639 (batch time: 22.5340)\n",
            "[50000/50000] loss: 2.8901 (batch time: 22.4800)\n",
            "Time elapsed: 45.039 seconds\n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.7921 (batch time: 21.2052)\n",
            "[50000/50000] loss: 2.7914 (batch time: 22.3840)\n",
            "Time elapsed: 43.6137 seconds\n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.7213 (batch time: 22.2415)\n",
            "[50000/50000] loss: 2.7600 (batch time: 21.3485)\n",
            "Test Error: \n",
            "Accuracy: 81.7%, Avg loss: 0.527426\n",
            "Time elapsed: 47.8379 seconds\n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.6979 (batch time: 21.8999)\n",
            "[50000/50000] loss: 2.6826 (batch time: 22.3615)\n",
            "Time elapsed: 44.2866 seconds\n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.6001 (batch time: 21.0793)\n",
            "[50000/50000] loss: 2.6575 (batch time: 22.3789)\n",
            "Time elapsed: 43.4833 seconds\n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.5598 (batch time: 22.3754)\n",
            "[50000/50000] loss: 2.5679 (batch time: 20.9857)\n",
            "Time elapsed: 43.386 seconds\n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.5078 (batch time: 22.5618)\n",
            "[50000/50000] loss: 2.4707 (batch time: 21.1037)\n",
            "Time elapsed: 43.7006 seconds\n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.4576 (batch time: 22.3015)\n",
            "[50000/50000] loss: 2.4413 (batch time: 22.7465)\n",
            "Time elapsed: 45.0734 seconds\n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.4128 (batch time: 21.1470)\n",
            "[50000/50000] loss: 2.3758 (batch time: 22.3607)\n",
            "Time elapsed: 43.533 seconds\n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.2916 (batch time: 22.5475)\n",
            "[50000/50000] loss: 2.3940 (batch time: 21.0027)\n",
            "Time elapsed: 43.5773 seconds\n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.3315 (batch time: 22.2994)\n",
            "[50000/50000] loss: 2.3012 (batch time: 21.3969)\n",
            "Time elapsed: 43.7317 seconds\n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.2282 (batch time: 22.0221)\n",
            "[50000/50000] loss: 2.2481 (batch time: 22.4186)\n",
            "Time elapsed: 44.4652 seconds\n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.2114 (batch time: 21.1158)\n",
            "[50000/50000] loss: 2.1960 (batch time: 22.3024)\n",
            "Test Error: \n",
            "Accuracy: 83.8%, Avg loss: 0.477711\n",
            "Time elapsed: 47.0883 seconds\n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.1057 (batch time: 22.2523)\n",
            "[50000/50000] loss: 2.1538 (batch time: 21.2718)\n",
            "Time elapsed: 43.5586 seconds\n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.0972 (batch time: 21.7392)\n",
            "[50000/50000] loss: 2.0702 (batch time: 22.1303)\n",
            "Time elapsed: 43.893 seconds\n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.0184 (batch time: 21.2321)\n",
            "[50000/50000] loss: 2.0342 (batch time: 22.2544)\n",
            "Time elapsed: 43.5115 seconds\n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "[25000/50000] loss: 2.0110 (batch time: 22.1458)\n",
            "[50000/50000] loss: 1.9993 (batch time: 21.5113)\n",
            "Time elapsed: 43.6808 seconds\n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.9850 (batch time: 22.5197)\n",
            "[50000/50000] loss: 1.9602 (batch time: 21.0042)\n",
            "Time elapsed: 43.5479 seconds\n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.9426 (batch time: 22.0830)\n",
            "[50000/50000] loss: 1.8864 (batch time: 22.2355)\n",
            "Time elapsed: 44.3434 seconds\n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.8864 (batch time: 21.0747)\n",
            "[50000/50000] loss: 1.8821 (batch time: 22.1617)\n",
            "Time elapsed: 43.2627 seconds\n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.8224 (batch time: 21.5726)\n",
            "[50000/50000] loss: 1.8447 (batch time: 21.9393)\n",
            "Time elapsed: 43.5353 seconds\n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.7800 (batch time: 22.0504)\n",
            "[50000/50000] loss: 1.8005 (batch time: 21.0430)\n",
            "Time elapsed: 43.1211 seconds\n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.7321 (batch time: 22.2323)\n",
            "[50000/50000] loss: 1.7856 (batch time: 21.5447)\n",
            "Test Error: \n",
            "Accuracy: 85.7%, Avg loss: 0.432521\n",
            "Time elapsed: 47.7453 seconds\n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.7117 (batch time: 21.3380)\n",
            "[50000/50000] loss: 1.7019 (batch time: 21.7935)\n",
            "Time elapsed: 43.1625 seconds\n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.6559 (batch time: 22.3936)\n",
            "[50000/50000] loss: 1.7209 (batch time: 20.9469)\n",
            "Time elapsed: 43.365 seconds\n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.5985 (batch time: 21.7399)\n",
            "[50000/50000] loss: 1.6590 (batch time: 20.6593)\n",
            "Time elapsed: 42.4337 seconds\n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.6028 (batch time: 21.6749)\n",
            "[50000/50000] loss: 1.5992 (batch time: 21.9875)\n",
            "Time elapsed: 43.6854 seconds\n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.5438 (batch time: 20.6481)\n",
            "[50000/50000] loss: 1.5674 (batch time: 21.9187)\n",
            "Time elapsed: 42.5904 seconds\n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.5062 (batch time: 21.0532)\n",
            "[50000/50000] loss: 1.5451 (batch time: 21.8298)\n",
            "Time elapsed: 42.9061 seconds\n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.4843 (batch time: 21.9742)\n",
            "[50000/50000] loss: 1.4832 (batch time: 20.8544)\n",
            "Time elapsed: 42.853 seconds\n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.4244 (batch time: 21.9788)\n",
            "[50000/50000] loss: 1.4558 (batch time: 20.6451)\n",
            "Time elapsed: 42.6683 seconds\n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.4298 (batch time: 21.9188)\n",
            "[50000/50000] loss: 1.4136 (batch time: 21.8439)\n",
            "Time elapsed: 43.7869 seconds\n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.3909 (batch time: 20.8008)\n",
            "[50000/50000] loss: 1.3892 (batch time: 21.9320)\n",
            "Test Error: \n",
            "Accuracy: 86.8%, Avg loss: 0.411297\n",
            "Time elapsed: 46.3006 seconds\n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.3360 (batch time: 21.8063)\n",
            "[50000/50000] loss: 1.3731 (batch time: 20.6413)\n",
            "Time elapsed: 42.4722 seconds\n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.3248 (batch time: 21.9342)\n",
            "[50000/50000] loss: 1.3235 (batch time: 20.7958)\n",
            "Time elapsed: 42.7671 seconds\n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.2892 (batch time: 21.5512)\n",
            "[50000/50000] loss: 1.2958 (batch time: 21.7667)\n",
            "Time elapsed: 43.3458 seconds\n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.2623 (batch time: 20.5854)\n",
            "[50000/50000] loss: 1.2616 (batch time: 21.9071)\n",
            "Time elapsed: 42.5162 seconds\n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.2141 (batch time: 20.8638)\n",
            "[50000/50000] loss: 1.2808 (batch time: 21.6927)\n",
            "Time elapsed: 42.5892 seconds\n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.1815 (batch time: 22.0042)\n",
            "[50000/50000] loss: 1.2410 (batch time: 20.4827)\n",
            "Time elapsed: 42.5107 seconds\n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.1676 (batch time: 21.7505)\n",
            "[50000/50000] loss: 1.1695 (batch time: 20.5194)\n",
            "Time elapsed: 42.2934 seconds\n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.1593 (batch time: 21.8431)\n",
            "[50000/50000] loss: 1.1746 (batch time: 21.3349)\n",
            "Time elapsed: 43.2147 seconds\n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.0885 (batch time: 20.6921)\n",
            "[50000/50000] loss: 1.1461 (batch time: 21.8016)\n",
            "Time elapsed: 42.5184 seconds\n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.0872 (batch time: 20.9485)\n",
            "[50000/50000] loss: 1.1147 (batch time: 22.0109)\n",
            "Test Error: \n",
            "Accuracy: 87.4%, Avg loss: 0.404039\n",
            "Time elapsed: 46.6087 seconds\n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.0848 (batch time: 21.7924)\n",
            "[50000/50000] loss: 1.0997 (batch time: 21.0944)\n",
            "Time elapsed: 42.925 seconds\n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.0066 (batch time: 21.5579)\n",
            "[50000/50000] loss: 1.0914 (batch time: 22.1524)\n",
            "Time elapsed: 43.7345 seconds\n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.0197 (batch time: 21.1674)\n",
            "[50000/50000] loss: 1.0310 (batch time: 21.9655)\n",
            "Time elapsed: 43.1565 seconds\n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "[25000/50000] loss: 1.0149 (batch time: 22.6727)\n",
            "[50000/50000] loss: 1.0194 (batch time: 20.7379)\n",
            "Time elapsed: 43.4352 seconds\n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.9818 (batch time: 21.8374)\n",
            "[50000/50000] loss: 0.9955 (batch time: 20.7471)\n",
            "Time elapsed: 42.6159 seconds\n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.9352 (batch time: 21.9048)\n",
            "[50000/50000] loss: 0.9853 (batch time: 21.6752)\n",
            "Time elapsed: 43.6145 seconds\n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.8852 (batch time: 20.9422)\n",
            "[50000/50000] loss: 0.9608 (batch time: 22.0037)\n",
            "Time elapsed: 42.9706 seconds\n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.9045 (batch time: 20.5307)\n",
            "[50000/50000] loss: 0.9221 (batch time: 22.0317)\n",
            "Time elapsed: 42.5923 seconds\n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.8675 (batch time: 21.4440)\n",
            "[50000/50000] loss: 0.8951 (batch time: 21.2292)\n",
            "Time elapsed: 42.7044 seconds\n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.8522 (batch time: 21.9926)\n",
            "[50000/50000] loss: 0.8637 (batch time: 21.0106)\n",
            "Test Error: \n",
            "Accuracy: 87.8%, Avg loss: 0.412856\n",
            "Time elapsed: 47.8479 seconds\n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.8206 (batch time: 20.9529)\n",
            "[50000/50000] loss: 0.8542 (batch time: 21.9040)\n",
            "Time elapsed: 42.8825 seconds\n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.7903 (batch time: 21.3026)\n",
            "[50000/50000] loss: 0.8156 (batch time: 21.7976)\n",
            "Time elapsed: 43.1346 seconds\n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.7706 (batch time: 22.0593)\n",
            "[50000/50000] loss: 0.8064 (batch time: 20.8654)\n",
            "Time elapsed: 42.9503 seconds\n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.7822 (batch time: 22.2291)\n",
            "[50000/50000] loss: 0.7778 (batch time: 21.2220)\n",
            "Time elapsed: 43.4856 seconds\n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.7470 (batch time: 21.6918)\n",
            "[50000/50000] loss: 0.7538 (batch time: 22.1162)\n",
            "Time elapsed: 43.8322 seconds\n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.7367 (batch time: 20.8213)\n",
            "[50000/50000] loss: 0.7635 (batch time: 22.3143)\n",
            "Time elapsed: 43.1598 seconds\n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.7385 (batch time: 21.3385)\n",
            "[50000/50000] loss: 0.7280 (batch time: 21.8549)\n",
            "Time elapsed: 43.2189 seconds\n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.6744 (batch time: 22.2922)\n",
            "[50000/50000] loss: 0.7319 (batch time: 20.7555)\n",
            "Time elapsed: 43.0734 seconds\n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.6870 (batch time: 21.9643)\n",
            "[50000/50000] loss: 0.7080 (batch time: 21.6940)\n",
            "Time elapsed: 43.6976 seconds\n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.6281 (batch time: 21.2530)\n",
            "[50000/50000] loss: 0.6942 (batch time: 21.8031)\n",
            "Test Error: \n",
            "Accuracy: 88.0%, Avg loss: 0.438036\n",
            "Time elapsed: 46.8211 seconds\n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.6176 (batch time: 22.1801)\n",
            "[50000/50000] loss: 0.6824 (batch time: 23.0396)\n",
            "Time elapsed: 45.2557 seconds\n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.6388 (batch time: 23.1137)\n",
            "[50000/50000] loss: 0.6521 (batch time: 23.5399)\n",
            "Time elapsed: 46.6837 seconds\n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.6065 (batch time: 22.3055)\n",
            "[50000/50000] loss: 0.6153 (batch time: 22.3610)\n",
            "Time elapsed: 44.6939 seconds\n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.6003 (batch time: 22.5860)\n",
            "[50000/50000] loss: 0.6138 (batch time: 21.0407)\n",
            "Time elapsed: 43.6561 seconds\n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.5903 (batch time: 22.8761)\n",
            "[50000/50000] loss: 0.5927 (batch time: 22.7843)\n",
            "Time elapsed: 45.6861 seconds\n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.5750 (batch time: 22.0154)\n",
            "[50000/50000] loss: 0.5744 (batch time: 22.3900)\n",
            "Time elapsed: 44.4291 seconds\n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.5837 (batch time: 22.8837)\n",
            "[50000/50000] loss: 0.6151 (batch time: 21.5824)\n",
            "Time elapsed: 44.4927 seconds\n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.5572 (batch time: 22.1597)\n",
            "[50000/50000] loss: 0.5548 (batch time: 22.5675)\n",
            "Time elapsed: 44.7502 seconds\n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.5220 (batch time: 22.0616)\n",
            "[50000/50000] loss: 0.5336 (batch time: 22.7876)\n",
            "Time elapsed: 44.8717 seconds\n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4839 (batch time: 22.1140)\n",
            "[50000/50000] loss: 0.5412 (batch time: 22.1929)\n",
            "Test Error: \n",
            "Accuracy: 88.0%, Avg loss: 0.468191\n",
            "Time elapsed: 47.9913 seconds\n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4867 (batch time: 22.1411)\n",
            "[50000/50000] loss: 0.5300 (batch time: 22.0504)\n",
            "Time elapsed: 44.2165 seconds\n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4806 (batch time: 20.7959)\n",
            "[50000/50000] loss: 0.5481 (batch time: 21.8995)\n",
            "Time elapsed: 42.7219 seconds\n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4951 (batch time: 20.7801)\n",
            "[50000/50000] loss: 0.4966 (batch time: 21.5608)\n",
            "Time elapsed: 42.3648 seconds\n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4448 (batch time: 22.0521)\n",
            "[50000/50000] loss: 0.4945 (batch time: 20.8611)\n",
            "Time elapsed: 42.9361 seconds\n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4589 (batch time: 21.9090)\n",
            "[50000/50000] loss: 0.4508 (batch time: 20.8399)\n",
            "Time elapsed: 42.7744 seconds\n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4503 (batch time: 21.4546)\n",
            "[50000/50000] loss: 0.4708 (batch time: 21.8949)\n",
            "Time elapsed: 43.3719 seconds\n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4330 (batch time: 20.8063)\n",
            "[50000/50000] loss: 0.4533 (batch time: 21.8053)\n",
            "Time elapsed: 42.6352 seconds\n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4044 (batch time: 21.1130)\n",
            "[50000/50000] loss: 0.4215 (batch time: 22.1268)\n",
            "Time elapsed: 43.2633 seconds\n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4104 (batch time: 21.9722)\n",
            "[50000/50000] loss: 0.4313 (batch time: 20.6458)\n",
            "Time elapsed: 42.6409 seconds\n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4044 (batch time: 22.0174)\n",
            "[50000/50000] loss: 0.4025 (batch time: 20.6225)\n",
            "Test Error: \n",
            "Accuracy: 88.1%, Avg loss: 0.481636\n",
            "Time elapsed: 47.6073 seconds\n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4121 (batch time: 20.8045)\n",
            "[50000/50000] loss: 0.4137 (batch time: 22.2862)\n",
            "Time elapsed: 43.117 seconds\n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4268 (batch time: 21.7250)\n",
            "[50000/50000] loss: 0.3861 (batch time: 21.2537)\n",
            "Time elapsed: 43.001 seconds\n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.4059 (batch time: 21.8498)\n",
            "[50000/50000] loss: 0.3934 (batch time: 20.6432)\n",
            "Time elapsed: 42.5203 seconds\n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3578 (batch time: 21.8415)\n",
            "[50000/50000] loss: 0.3983 (batch time: 20.8526)\n",
            "Time elapsed: 42.7348 seconds\n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3793 (batch time: 21.5741)\n",
            "[50000/50000] loss: 0.3721 (batch time: 21.8533)\n",
            "Time elapsed: 43.4544 seconds\n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3661 (batch time: 20.5208)\n",
            "[50000/50000] loss: 0.3577 (batch time: 21.7195)\n",
            "Time elapsed: 42.2644 seconds\n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3496 (batch time: 20.6475)\n",
            "[50000/50000] loss: 0.3800 (batch time: 21.6602)\n",
            "Time elapsed: 42.3319 seconds\n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3160 (batch time: 21.5083)\n",
            "[50000/50000] loss: 0.3623 (batch time: 20.6863)\n",
            "Time elapsed: 42.2254 seconds\n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3502 (batch time: 21.8451)\n",
            "[50000/50000] loss: 0.3322 (batch time: 20.6587)\n",
            "Time elapsed: 42.5304 seconds\n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3252 (batch time: 21.6475)\n",
            "[50000/50000] loss: 0.3206 (batch time: 21.1400)\n",
            "Test Error: \n",
            "Accuracy: 88.2%, Avg loss: 0.527099\n",
            "Time elapsed: 46.7133 seconds\n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3061 (batch time: 20.8425)\n",
            "[50000/50000] loss: 0.3614 (batch time: 21.5160)\n",
            "Time elapsed: 42.3849 seconds\n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2994 (batch time: 21.8803)\n",
            "[50000/50000] loss: 0.3245 (batch time: 20.7274)\n",
            "Time elapsed: 42.6311 seconds\n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2850 (batch time: 21.7898)\n",
            "[50000/50000] loss: 0.3172 (batch time: 20.6680)\n",
            "Time elapsed: 42.4831 seconds\n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.3000 (batch time: 21.8602)\n",
            "[50000/50000] loss: 0.2907 (batch time: 21.7876)\n",
            "Time elapsed: 43.6719 seconds\n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2926 (batch time: 20.6953)\n",
            "[50000/50000] loss: 0.3040 (batch time: 21.7659)\n",
            "Time elapsed: 42.4868 seconds\n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2840 (batch time: 20.6876)\n",
            "[50000/50000] loss: 0.2842 (batch time: 22.0367)\n",
            "Time elapsed: 42.7478 seconds\n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2891 (batch time: 21.5805)\n",
            "[50000/50000] loss: 0.3012 (batch time: 20.8593)\n",
            "Time elapsed: 42.4649 seconds\n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2633 (batch time: 22.0873)\n",
            "[50000/50000] loss: 0.3088 (batch time: 20.7211)\n",
            "Time elapsed: 42.8325 seconds\n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2691 (batch time: 21.9584)\n",
            "[50000/50000] loss: 0.2722 (batch time: 21.4243)\n",
            "Time elapsed: 43.4153 seconds\n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2511 (batch time: 21.0649)\n",
            "[50000/50000] loss: 0.2987 (batch time: 21.8781)\n",
            "Test Error: \n",
            "Accuracy: 88.5%, Avg loss: 0.506278\n",
            "Time elapsed: 46.5925 seconds\n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2580 (batch time: 21.9455)\n",
            "[50000/50000] loss: 0.2603 (batch time: 20.6594)\n",
            "Time elapsed: 42.6296 seconds\n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2404 (batch time: 21.9744)\n",
            "[50000/50000] loss: 0.2588 (batch time: 20.5499)\n",
            "Time elapsed: 42.5491 seconds\n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2491 (batch time: 21.9973)\n",
            "[50000/50000] loss: 0.2580 (batch time: 22.0588)\n",
            "Time elapsed: 44.0793 seconds\n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2368 (batch time: 20.8359)\n",
            "[50000/50000] loss: 0.2546 (batch time: 21.9975)\n",
            "Time elapsed: 42.8579 seconds\n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2261 (batch time: 21.0922)\n",
            "[50000/50000] loss: 0.2687 (batch time: 21.8661)\n",
            "Time elapsed: 42.9805 seconds\n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2398 (batch time: 21.7593)\n",
            "[50000/50000] loss: 0.2385 (batch time: 20.6728)\n",
            "Time elapsed: 42.4578 seconds\n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2165 (batch time: 22.0249)\n",
            "[50000/50000] loss: 0.2427 (batch time: 20.8187)\n",
            "Time elapsed: 42.8891 seconds\n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2228 (batch time: 21.6736)\n",
            "[50000/50000] loss: 0.2585 (batch time: 21.8420)\n",
            "Time elapsed: 43.5407 seconds\n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2179 (batch time: 20.6775)\n",
            "[50000/50000] loss: 0.2427 (batch time: 22.0208)\n",
            "Time elapsed: 42.7212 seconds\n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2144 (batch time: 20.8114)\n",
            "[50000/50000] loss: 0.2270 (batch time: 21.7688)\n",
            "Test Error: \n",
            "Accuracy: 88.5%, Avg loss: 0.555304\n",
            "Time elapsed: 46.2175 seconds\n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2227 (batch time: 21.7389)\n",
            "[50000/50000] loss: 0.2396 (batch time: 21.1473)\n",
            "Time elapsed: 42.9206 seconds\n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1936 (batch time: 21.2763)\n",
            "[50000/50000] loss: 0.2205 (batch time: 21.8717)\n",
            "Time elapsed: 43.1734 seconds\n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1895 (batch time: 20.7398)\n",
            "[50000/50000] loss: 0.2166 (batch time: 21.8089)\n",
            "Time elapsed: 42.5722 seconds\n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2025 (batch time: 21.0789)\n",
            "[50000/50000] loss: 0.2032 (batch time: 21.4066)\n",
            "Time elapsed: 42.5106 seconds\n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1926 (batch time: 21.8936)\n",
            "[50000/50000] loss: 0.1853 (batch time: 20.5448)\n",
            "Time elapsed: 42.4651 seconds\n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1883 (batch time: 22.0625)\n",
            "[50000/50000] loss: 0.2014 (batch time: 20.6755)\n",
            "Time elapsed: 42.7735 seconds\n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1953 (batch time: 21.7013)\n",
            "[50000/50000] loss: 0.2077 (batch time: 22.0270)\n",
            "Time elapsed: 43.7537 seconds\n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.2008 (batch time: 20.7065)\n",
            "[50000/50000] loss: 0.2070 (batch time: 21.5894)\n",
            "Time elapsed: 42.3187 seconds\n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1834 (batch time: 20.9312)\n",
            "[50000/50000] loss: 0.1809 (batch time: 21.3258)\n",
            "Time elapsed: 42.2817 seconds\n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1701 (batch time: 21.9303)\n",
            "[50000/50000] loss: 0.1881 (batch time: 20.7359)\n",
            "Test Error: \n",
            "Accuracy: 88.4%, Avg loss: 0.595345\n",
            "Time elapsed: 47.1052 seconds\n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1860 (batch time: 20.7284)\n",
            "[50000/50000] loss: 0.1768 (batch time: 21.8844)\n",
            "Time elapsed: 42.6369 seconds\n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1663 (batch time: 20.6045)\n",
            "[50000/50000] loss: 0.1874 (batch time: 21.9567)\n",
            "Time elapsed: 42.5854 seconds\n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1616 (batch time: 21.7390)\n",
            "[50000/50000] loss: 0.1978 (batch time: 20.8443)\n",
            "Time elapsed: 42.6065 seconds\n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1582 (batch time: 21.9835)\n",
            "[50000/50000] loss: 0.1837 (batch time: 20.7137)\n",
            "Time elapsed: 42.7214 seconds\n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1678 (batch time: 21.8958)\n",
            "[50000/50000] loss: 0.1858 (batch time: 20.7911)\n",
            "Time elapsed: 42.7203 seconds\n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1728 (batch time: 21.5546)\n",
            "[50000/50000] loss: 0.1786 (batch time: 21.8806)\n",
            "Time elapsed: 43.4584 seconds\n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1530 (batch time: 20.3769)\n",
            "[50000/50000] loss: 0.1681 (batch time: 21.5359)\n",
            "Time elapsed: 41.94 seconds\n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1673 (batch time: 20.7940)\n",
            "[50000/50000] loss: 0.1757 (batch time: 21.4885)\n",
            "Time elapsed: 42.3058 seconds\n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1647 (batch time: 21.6964)\n",
            "[50000/50000] loss: 0.1625 (batch time: 20.5104)\n",
            "Time elapsed: 42.232 seconds\n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1738 (batch time: 21.9593)\n",
            "[50000/50000] loss: 0.1680 (batch time: 20.2914)\n",
            "Test Error: \n",
            "Accuracy: 89.3%, Avg loss: 0.517679\n",
            "Time elapsed: 47.0758 seconds\n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1488 (batch time: 20.5653)\n",
            "[50000/50000] loss: 0.1660 (batch time: 21.6189)\n",
            "Time elapsed: 42.2092 seconds\n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1548 (batch time: 20.6414)\n",
            "[50000/50000] loss: 0.1490 (batch time: 21.3259)\n",
            "Time elapsed: 41.994 seconds\n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1406 (batch time: 21.7361)\n",
            "[50000/50000] loss: 0.1473 (batch time: 20.4433)\n",
            "Time elapsed: 42.2047 seconds\n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1469 (batch time: 21.6534)\n",
            "[50000/50000] loss: 0.1583 (batch time: 20.4681)\n",
            "Time elapsed: 42.1474 seconds\n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1353 (batch time: 21.5504)\n",
            "[50000/50000] loss: 0.1482 (batch time: 20.8122)\n",
            "Time elapsed: 42.399 seconds\n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1341 (batch time: 21.1381)\n",
            "[50000/50000] loss: 0.1648 (batch time: 21.5335)\n",
            "Time elapsed: 42.6971 seconds\n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1499 (batch time: 20.5278)\n",
            "[50000/50000] loss: 0.1524 (batch time: 21.4972)\n",
            "Time elapsed: 42.0495 seconds\n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1251 (batch time: 20.7415)\n",
            "[50000/50000] loss: 0.1472 (batch time: 21.5436)\n",
            "Time elapsed: 42.3101 seconds\n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1155 (batch time: 21.4314)\n",
            "[50000/50000] loss: 0.1472 (batch time: 20.7551)\n",
            "Time elapsed: 42.2084 seconds\n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1382 (batch time: 21.6626)\n",
            "[50000/50000] loss: 0.1485 (batch time: 20.4818)\n",
            "Test Error: \n",
            "Accuracy: 88.9%, Avg loss: 0.577119\n",
            "Time elapsed: 46.9929 seconds\n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1355 (batch time: 20.5123)\n",
            "[50000/50000] loss: 0.1268 (batch time: 21.6539)\n",
            "Time elapsed: 42.1901 seconds\n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1226 (batch time: 20.4497)\n",
            "[50000/50000] loss: 0.1352 (batch time: 21.3507)\n",
            "Time elapsed: 41.8235 seconds\n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1197 (batch time: 21.2307)\n",
            "[50000/50000] loss: 0.1166 (batch time: 21.1075)\n",
            "Time elapsed: 42.3603 seconds\n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1332 (batch time: 21.6254)\n",
            "[50000/50000] loss: 0.1228 (batch time: 20.3479)\n",
            "Time elapsed: 41.9975 seconds\n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1289 (batch time: 21.7378)\n",
            "[50000/50000] loss: 0.1231 (batch time: 20.3739)\n",
            "Time elapsed: 42.1357 seconds\n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1303 (batch time: 21.4865)\n",
            "[50000/50000] loss: 0.1296 (batch time: 21.3871)\n",
            "Time elapsed: 42.897 seconds\n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1081 (batch time: 20.5791)\n",
            "[50000/50000] loss: 0.1291 (batch time: 21.7490)\n",
            "Time elapsed: 42.3536 seconds\n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1144 (batch time: 20.4418)\n",
            "[50000/50000] loss: 0.1219 (batch time: 21.5665)\n",
            "Time elapsed: 42.032 seconds\n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1214 (batch time: 20.6787)\n",
            "[50000/50000] loss: 0.1157 (batch time: 21.2294)\n",
            "Time elapsed: 41.9308 seconds\n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1077 (batch time: 21.5338)\n",
            "[50000/50000] loss: 0.1281 (batch time: 20.2541)\n",
            "Test Error: \n",
            "Accuracy: 88.6%, Avg loss: 0.624473\n",
            "Time elapsed: 45.6164 seconds\n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1153 (batch time: 21.3276)\n",
            "[50000/50000] loss: 0.1337 (batch time: 21.4505)\n",
            "Time elapsed: 42.8034 seconds\n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1106 (batch time: 20.3992)\n",
            "[50000/50000] loss: 0.1399 (batch time: 21.5789)\n",
            "Time elapsed: 42.0028 seconds\n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1079 (batch time: 20.4771)\n",
            "[50000/50000] loss: 0.1221 (batch time: 21.5778)\n",
            "Time elapsed: 42.079 seconds\n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1086 (batch time: 20.9573)\n",
            "[50000/50000] loss: 0.1183 (batch time: 21.0112)\n",
            "Time elapsed: 41.999 seconds\n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1269 (batch time: 21.7469)\n",
            "[50000/50000] loss: 0.1182 (batch time: 20.5076)\n",
            "Time elapsed: 42.2792 seconds\n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1109 (batch time: 21.6727)\n",
            "[50000/50000] loss: 0.1158 (batch time: 20.5836)\n",
            "Time elapsed: 42.2879 seconds\n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1161 (batch time: 21.4849)\n",
            "[50000/50000] loss: 0.0944 (batch time: 21.5635)\n",
            "Time elapsed: 43.0733 seconds\n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1120 (batch time: 20.4240)\n",
            "[50000/50000] loss: 0.0986 (batch time: 21.5396)\n",
            "Time elapsed: 41.9906 seconds\n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1135 (batch time: 20.6196)\n",
            "[50000/50000] loss: 0.0959 (batch time: 21.5401)\n",
            "Time elapsed: 42.1836 seconds\n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0767 (batch time: 21.2049)\n",
            "[50000/50000] loss: 0.1084 (batch time: 21.0198)\n",
            "Test Error: \n",
            "Accuracy: 89.3%, Avg loss: 0.578815\n",
            "Time elapsed: 45.8826 seconds\n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0927 (batch time: 21.9344)\n",
            "[50000/50000] loss: 0.1111 (batch time: 21.4274)\n",
            "Time elapsed: 43.3962 seconds\n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.1156 (batch time: 20.6330)\n",
            "[50000/50000] loss: 0.1029 (batch time: 21.7796)\n",
            "Time elapsed: 42.4387 seconds\n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0980 (batch time: 20.6443)\n",
            "[50000/50000] loss: 0.1069 (batch time: 21.6019)\n",
            "Time elapsed: 42.2703 seconds\n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0979 (batch time: 20.8722)\n",
            "[50000/50000] loss: 0.0865 (batch time: 21.2738)\n",
            "Time elapsed: 42.1682 seconds\n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0925 (batch time: 21.6338)\n",
            "[50000/50000] loss: 0.1257 (batch time: 20.5313)\n",
            "Time elapsed: 42.1897 seconds\n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0772 (batch time: 21.7818)\n",
            "[50000/50000] loss: 0.1142 (batch time: 20.5483)\n",
            "Time elapsed: 42.3544 seconds\n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0872 (batch time: 21.4011)\n",
            "[50000/50000] loss: 0.0689 (batch time: 20.8044)\n",
            "Time elapsed: 42.2403 seconds\n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0893 (batch time: 20.7671)\n",
            "[50000/50000] loss: 0.0905 (batch time: 21.6695)\n",
            "Time elapsed: 42.4595 seconds\n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0972 (batch time: 20.5107)\n",
            "[50000/50000] loss: 0.1155 (batch time: 21.5868)\n",
            "Time elapsed: 42.121 seconds\n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0973 (batch time: 20.3790)\n",
            "[50000/50000] loss: 0.0853 (batch time: 21.3381)\n",
            "Test Error: \n",
            "Accuracy: 89.1%, Avg loss: 0.607591\n",
            "Time elapsed: 45.6814 seconds\n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0864 (batch time: 21.6435)\n",
            "[50000/50000] loss: 0.0777 (batch time: 20.7569)\n",
            "Time elapsed: 42.4403 seconds\n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0976 (batch time: 21.3496)\n",
            "[50000/50000] loss: 0.1111 (batch time: 21.4058)\n",
            "Time elapsed: 42.7829 seconds\n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0841 (batch time: 20.4511)\n",
            "[50000/50000] loss: 0.0858 (batch time: 21.5661)\n",
            "Time elapsed: 42.0433 seconds\n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0763 (batch time: 20.3508)\n",
            "[50000/50000] loss: 0.0950 (batch time: 21.5751)\n",
            "Time elapsed: 41.9499 seconds\n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0859 (batch time: 20.9458)\n",
            "[50000/50000] loss: 0.0922 (batch time: 20.4873)\n",
            "Time elapsed: 41.4571 seconds\n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0701 (batch time: 21.5442)\n",
            "[50000/50000] loss: 0.1047 (batch time: 20.1206)\n",
            "Time elapsed: 41.6911 seconds\n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0897 (batch time: 21.2375)\n",
            "[50000/50000] loss: 0.0885 (batch time: 20.3975)\n",
            "Time elapsed: 41.66 seconds\n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0928 (batch time: 21.5743)\n",
            "[50000/50000] loss: 0.0788 (batch time: 21.1117)\n",
            "Time elapsed: 42.7203 seconds\n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0833 (batch time: 20.5234)\n",
            "[50000/50000] loss: 0.0971 (batch time: 21.6895)\n",
            "Time elapsed: 42.2393 seconds\n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0822 (batch time: 20.4009)\n",
            "[50000/50000] loss: 0.0830 (batch time: 21.4329)\n",
            "Test Error: \n",
            "Accuracy: 89.1%, Avg loss: 0.629294\n",
            "Time elapsed: 45.3278 seconds\n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0742 (batch time: 21.6098)\n",
            "[50000/50000] loss: 0.0688 (batch time: 20.2837)\n",
            "Time elapsed: 41.9202 seconds\n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0677 (batch time: 21.5010)\n",
            "[50000/50000] loss: 0.0885 (batch time: 20.6651)\n",
            "Time elapsed: 42.2043 seconds\n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0771 (batch time: 21.1905)\n",
            "[50000/50000] loss: 0.0905 (batch time: 21.3665)\n",
            "Time elapsed: 42.5815 seconds\n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0810 (batch time: 20.5138)\n",
            "[50000/50000] loss: 0.0823 (batch time: 21.6767)\n",
            "Time elapsed: 42.2168 seconds\n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0924 (batch time: 20.4154)\n",
            "[50000/50000] loss: 0.0798 (batch time: 21.6713)\n",
            "Time elapsed: 42.114 seconds\n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0835 (batch time: 21.0612)\n",
            "[50000/50000] loss: 0.0672 (batch time: 21.1915)\n",
            "Time elapsed: 42.28 seconds\n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0910 (batch time: 21.7488)\n",
            "[50000/50000] loss: 0.0780 (batch time: 20.5358)\n",
            "Time elapsed: 42.3106 seconds\n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0770 (batch time: 21.9852)\n",
            "[50000/50000] loss: 0.0723 (batch time: 20.7008)\n",
            "Time elapsed: 42.7118 seconds\n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0641 (batch time: 22.0027)\n",
            "[50000/50000] loss: 0.0824 (batch time: 21.2802)\n",
            "Time elapsed: 43.3201 seconds\n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0737 (batch time: 20.8542)\n",
            "[50000/50000] loss: 0.0756 (batch time: 21.7242)\n",
            "Test Error: \n",
            "Accuracy: 88.9%, Avg loss: 0.646981\n",
            "Time elapsed: 46.1914 seconds\n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0699 (batch time: 21.7479)\n",
            "[50000/50000] loss: 0.0823 (batch time: 20.3327)\n",
            "Time elapsed: 42.1075 seconds\n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0686 (batch time: 21.6478)\n",
            "[50000/50000] loss: 0.0854 (batch time: 20.3601)\n",
            "Time elapsed: 42.0333 seconds\n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0688 (batch time: 21.4861)\n",
            "[50000/50000] loss: 0.0617 (batch time: 21.7469)\n",
            "Time elapsed: 43.2688 seconds\n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0563 (batch time: 20.5066)\n",
            "[50000/50000] loss: 0.0641 (batch time: 21.9633)\n",
            "Time elapsed: 42.4938 seconds\n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0675 (batch time: 20.5748)\n",
            "[50000/50000] loss: 0.0653 (batch time: 21.3220)\n",
            "Time elapsed: 41.9225 seconds\n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0611 (batch time: 20.9977)\n",
            "[50000/50000] loss: 0.0785 (batch time: 21.0801)\n",
            "Time elapsed: 42.1035 seconds\n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0688 (batch time: 21.8360)\n",
            "[50000/50000] loss: 0.0726 (batch time: 20.3710)\n",
            "Time elapsed: 42.2316 seconds\n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0659 (batch time: 21.6625)\n",
            "[50000/50000] loss: 0.0686 (batch time: 20.3510)\n",
            "Time elapsed: 42.0395 seconds\n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0681 (batch time: 21.3881)\n",
            "[50000/50000] loss: 0.0724 (batch time: 21.0301)\n",
            "Time elapsed: 42.4537 seconds\n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0801 (batch time: 20.5109)\n",
            "[50000/50000] loss: 0.0638 (batch time: 21.2460)\n",
            "Test Error: \n",
            "Accuracy: 88.8%, Avg loss: 0.634531\n",
            "Time elapsed: 45.3592 seconds\n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0772 (batch time: 21.6045)\n",
            "[50000/50000] loss: 0.0755 (batch time: 20.4322)\n",
            "Time elapsed: 42.0671 seconds\n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0701 (batch time: 21.5243)\n",
            "[50000/50000] loss: 0.0763 (batch time: 20.2944)\n",
            "Time elapsed: 41.8443 seconds\n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0577 (batch time: 21.6365)\n",
            "[50000/50000] loss: 0.0766 (batch time: 20.6207)\n",
            "Time elapsed: 42.2922 seconds\n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0678 (batch time: 21.4734)\n",
            "[50000/50000] loss: 0.0599 (batch time: 21.4961)\n",
            "Time elapsed: 42.996 seconds\n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0613 (batch time: 20.6300)\n",
            "[50000/50000] loss: 0.0636 (batch time: 21.5648)\n",
            "Time elapsed: 42.2221 seconds\n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0657 (batch time: 20.4821)\n",
            "[50000/50000] loss: 0.0598 (batch time: 21.4505)\n",
            "Time elapsed: 41.9573 seconds\n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0767 (batch time: 20.9235)\n",
            "[50000/50000] loss: 0.0696 (batch time: 21.3640)\n",
            "Time elapsed: 42.3126 seconds\n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0754 (batch time: 21.6774)\n",
            "[50000/50000] loss: 0.0798 (batch time: 20.3093)\n",
            "Time elapsed: 42.0119 seconds\n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0722 (batch time: 21.2515)\n",
            "[50000/50000] loss: 0.0782 (batch time: 20.1283)\n",
            "Time elapsed: 41.4048 seconds\n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0556 (batch time: 21.7277)\n",
            "[50000/50000] loss: 0.0517 (batch time: 20.2986)\n",
            "Test Error: \n",
            "Accuracy: 89.2%, Avg loss: 0.655104\n",
            "Time elapsed: 46.7023 seconds\n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0649 (batch time: 20.5592)\n",
            "[50000/50000] loss: 0.0655 (batch time: 21.7186)\n",
            "Time elapsed: 42.3035 seconds\n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0596 (batch time: 21.4239)\n",
            "[50000/50000] loss: 0.0532 (batch time: 20.6456)\n",
            "Time elapsed: 42.1059 seconds\n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0448 (batch time: 21.6325)\n",
            "[50000/50000] loss: 0.0666 (batch time: 20.3408)\n",
            "Time elapsed: 42.0003 seconds\n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0492 (batch time: 21.7260)\n",
            "[50000/50000] loss: 0.0675 (batch time: 20.3491)\n",
            "Time elapsed: 42.1042 seconds\n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0498 (batch time: 21.5958)\n",
            "[50000/50000] loss: 0.0669 (batch time: 21.0041)\n",
            "Time elapsed: 42.6414 seconds\n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0657 (batch time: 20.4366)\n",
            "[50000/50000] loss: 0.0570 (batch time: 21.5201)\n",
            "Time elapsed: 41.9837 seconds\n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0580 (batch time: 20.4099)\n",
            "[50000/50000] loss: 0.0565 (batch time: 21.5158)\n",
            "Time elapsed: 41.9511 seconds\n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0606 (batch time: 20.3876)\n",
            "[50000/50000] loss: 0.0695 (batch time: 21.1367)\n",
            "Time elapsed: 41.5522 seconds\n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0585 (batch time: 21.4610)\n",
            "[50000/50000] loss: 0.0542 (batch time: 20.5725)\n",
            "Time elapsed: 42.0662 seconds\n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0601 (batch time: 22.0155)\n",
            "[50000/50000] loss: 0.0730 (batch time: 20.4260)\n",
            "Test Error: \n",
            "Accuracy: 89.4%, Avg loss: 0.671073\n",
            "Time elapsed: 47.3215 seconds\n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0455 (batch time: 20.5699)\n",
            "[50000/50000] loss: 0.0657 (batch time: 21.6452)\n",
            "Time elapsed: 42.2399 seconds\n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0525 (batch time: 20.3587)\n",
            "[50000/50000] loss: 0.0453 (batch time: 21.6456)\n",
            "Time elapsed: 42.0314 seconds\n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0584 (batch time: 20.5525)\n",
            "[50000/50000] loss: 0.0593 (batch time: 20.9673)\n",
            "Time elapsed: 41.5442 seconds\n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0588 (batch time: 21.5093)\n",
            "[50000/50000] loss: 0.0482 (batch time: 20.2530)\n",
            "Time elapsed: 41.7869 seconds\n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0490 (batch time: 21.5396)\n",
            "[50000/50000] loss: 0.0489 (batch time: 20.8289)\n",
            "Time elapsed: 42.3929 seconds\n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0435 (batch time: 21.9867)\n",
            "[50000/50000] loss: 0.0500 (batch time: 21.6974)\n",
            "Time elapsed: 43.7184 seconds\n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0578 (batch time: 20.5942)\n",
            "[50000/50000] loss: 0.0461 (batch time: 21.7401)\n",
            "Time elapsed: 42.3604 seconds\n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0540 (batch time: 20.4547)\n",
            "[50000/50000] loss: 0.0597 (batch time: 21.8291)\n",
            "Time elapsed: 42.3097 seconds\n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0730 (batch time: 21.2239)\n",
            "[50000/50000] loss: 0.0486 (batch time: 20.8903)\n",
            "Time elapsed: 42.1393 seconds\n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0628 (batch time: 21.8260)\n",
            "[50000/50000] loss: 0.0538 (batch time: 20.7082)\n",
            "Test Error: \n",
            "Accuracy: 88.9%, Avg loss: 0.646506\n",
            "Time elapsed: 47.5548 seconds\n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0590 (batch time: 20.9300)\n",
            "[50000/50000] loss: 0.0583 (batch time: 21.7103)\n",
            "Time elapsed: 42.6677 seconds\n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0515 (batch time: 21.0833)\n",
            "[50000/50000] loss: 0.0436 (batch time: 21.8128)\n",
            "Time elapsed: 42.9311 seconds\n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0514 (batch time: 21.9544)\n",
            "[50000/50000] loss: 0.0632 (batch time: 20.7660)\n",
            "Time elapsed: 42.7474 seconds\n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0570 (batch time: 22.1024)\n",
            "[50000/50000] loss: 0.0530 (batch time: 20.6836)\n",
            "Time elapsed: 42.8262 seconds\n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0490 (batch time: 21.6839)\n",
            "[50000/50000] loss: 0.0558 (batch time: 21.7262)\n",
            "Time elapsed: 43.4366 seconds\n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0450 (batch time: 20.5467)\n",
            "[50000/50000] loss: 0.0528 (batch time: 21.7063)\n",
            "Time elapsed: 42.2778 seconds\n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0531 (batch time: 20.4624)\n",
            "[50000/50000] loss: 0.0512 (batch time: 21.5340)\n",
            "Time elapsed: 42.0209 seconds\n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0557 (batch time: 21.2004)\n",
            "[50000/50000] loss: 0.0434 (batch time: 20.9373)\n",
            "Time elapsed: 42.1704 seconds\n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0493 (batch time: 21.7893)\n",
            "[50000/50000] loss: 0.0498 (batch time: 20.6645)\n",
            "Time elapsed: 42.4798 seconds\n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0388 (batch time: 21.8547)\n",
            "[50000/50000] loss: 0.0493 (batch time: 20.7567)\n",
            "Test Error: \n",
            "Accuracy: 89.0%, Avg loss: 0.692787\n",
            "Time elapsed: 47.1278 seconds\n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0518 (batch time: 20.6653)\n",
            "[50000/50000] loss: 0.0548 (batch time: 21.8892)\n",
            "Time elapsed: 42.5889 seconds\n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0528 (batch time: 21.3178)\n",
            "[50000/50000] loss: 0.0566 (batch time: 21.1283)\n",
            "Time elapsed: 42.4711 seconds\n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0475 (batch time: 21.8255)\n",
            "[50000/50000] loss: 0.0464 (batch time: 20.6396)\n",
            "Time elapsed: 42.493 seconds\n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0484 (batch time: 21.9820)\n",
            "[50000/50000] loss: 0.0506 (batch time: 20.8429)\n",
            "Time elapsed: 42.8604 seconds\n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0463 (batch time: 22.0001)\n",
            "[50000/50000] loss: 0.0480 (batch time: 21.8975)\n",
            "Time elapsed: 43.9223 seconds\n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0382 (batch time: 20.6583)\n",
            "[50000/50000] loss: 0.0574 (batch time: 21.5669)\n",
            "Time elapsed: 42.251 seconds\n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0548 (batch time: 20.9551)\n",
            "[50000/50000] loss: 0.0491 (batch time: 22.0920)\n",
            "Time elapsed: 43.0727 seconds\n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0495 (batch time: 22.2145)\n",
            "[50000/50000] loss: 0.0536 (batch time: 20.6301)\n",
            "Time elapsed: 42.8718 seconds\n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0493 (batch time: 22.0861)\n",
            "[50000/50000] loss: 0.0424 (batch time: 21.2690)\n",
            "Time elapsed: 43.397 seconds\n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0377 (batch time: 21.8492)\n",
            "[50000/50000] loss: 0.0511 (batch time: 22.1328)\n",
            "Test Error: \n",
            "Accuracy: 89.2%, Avg loss: 0.697866\n",
            "Time elapsed: 47.5804 seconds\n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0494 (batch time: 22.0984)\n",
            "[50000/50000] loss: 0.0519 (batch time: 20.7959)\n",
            "Time elapsed: 42.9186 seconds\n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0460 (batch time: 22.0875)\n",
            "[50000/50000] loss: 0.0511 (batch time: 20.7646)\n",
            "Time elapsed: 42.8795 seconds\n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0470 (batch time: 22.2804)\n",
            "[50000/50000] loss: 0.0445 (batch time: 22.0918)\n",
            "Time elapsed: 44.4002 seconds\n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0427 (batch time: 21.1355)\n",
            "[50000/50000] loss: 0.0533 (batch time: 22.0796)\n",
            "Time elapsed: 43.2433 seconds\n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0386 (batch time: 21.7529)\n",
            "[50000/50000] loss: 0.0551 (batch time: 21.0867)\n",
            "Time elapsed: 42.8701 seconds\n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0353 (batch time: 21.7726)\n",
            "[50000/50000] loss: 0.0395 (batch time: 20.5183)\n",
            "Time elapsed: 42.317 seconds\n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0557 (batch time: 21.7269)\n",
            "[50000/50000] loss: 0.0457 (batch time: 21.3082)\n",
            "Time elapsed: 43.0803 seconds\n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0424 (batch time: 20.6239)\n",
            "[50000/50000] loss: 0.0348 (batch time: 21.6999)\n",
            "Time elapsed: 42.3504 seconds\n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0356 (batch time: 20.4747)\n",
            "[50000/50000] loss: 0.0459 (batch time: 21.3633)\n",
            "Time elapsed: 41.865 seconds\n",
            "\n",
            "Epoch 301\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0323 (batch time: 20.9541)\n",
            "[50000/50000] loss: 0.0539 (batch time: 21.3865)\n",
            "Test Error: \n",
            "Accuracy: 89.3%, Avg loss: 0.674822\n",
            "Time elapsed: 45.9335 seconds\n",
            "\n",
            "Epoch 302\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0291 (batch time: 21.7377)\n",
            "[50000/50000] loss: 0.0390 (batch time: 20.8982)\n",
            "Time elapsed: 42.6725 seconds\n",
            "\n",
            "Epoch 303\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0406 (batch time: 21.5522)\n",
            "[50000/50000] loss: 0.0360 (batch time: 21.9692)\n",
            "Time elapsed: 43.5469 seconds\n",
            "\n",
            "Epoch 304\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0228 (batch time: 20.8067)\n",
            "[50000/50000] loss: 0.0457 (batch time: 21.6833)\n",
            "Time elapsed: 42.5174 seconds\n",
            "\n",
            "Epoch 305\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0426 (batch time: 21.0199)\n",
            "[50000/50000] loss: 0.0369 (batch time: 21.7015)\n",
            "Time elapsed: 42.7464 seconds\n",
            "\n",
            "Epoch 306\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0571 (batch time: 21.8349)\n",
            "[50000/50000] loss: 0.0462 (batch time: 20.4578)\n",
            "Time elapsed: 42.3195 seconds\n",
            "\n",
            "Epoch 307\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0446 (batch time: 21.8116)\n",
            "[50000/50000] loss: 0.0459 (batch time: 20.0319)\n",
            "Time elapsed: 41.8678 seconds\n",
            "\n",
            "Epoch 308\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0570 (batch time: 21.8054)\n",
            "[50000/50000] loss: 0.0367 (batch time: 21.2235)\n",
            "Time elapsed: 43.0676 seconds\n",
            "\n",
            "Epoch 309\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0356 (batch time: 20.5805)\n",
            "[50000/50000] loss: 0.0409 (batch time: 21.3873)\n",
            "Time elapsed: 41.9915 seconds\n",
            "\n",
            "Epoch 310\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0342 (batch time: 20.5080)\n",
            "[50000/50000] loss: 0.0420 (batch time: 21.8003)\n",
            "Time elapsed: 42.3338 seconds\n",
            "\n",
            "Epoch 311\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0439 (batch time: 20.9864)\n",
            "[50000/50000] loss: 0.0278 (batch time: 21.1073)\n",
            "Test Error: \n",
            "Accuracy: 89.6%, Avg loss: 0.702281\n",
            "Time elapsed: 45.6497 seconds\n",
            "\n",
            "Epoch 312\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0478 (batch time: 21.4344)\n",
            "[50000/50000] loss: 0.0314 (batch time: 21.0319)\n",
            "Time elapsed: 42.5025 seconds\n",
            "\n",
            "Epoch 313\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0349 (batch time: 20.3002)\n",
            "[50000/50000] loss: 0.0449 (batch time: 21.7923)\n",
            "Time elapsed: 42.1192 seconds\n",
            "\n",
            "Epoch 314\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0432 (batch time: 20.5761)\n",
            "[50000/50000] loss: 0.0624 (batch time: 21.7157)\n",
            "Time elapsed: 42.3183 seconds\n",
            "\n",
            "Epoch 315\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0421 (batch time: 21.6128)\n",
            "[50000/50000] loss: 0.0468 (batch time: 20.9574)\n",
            "Time elapsed: 42.6042 seconds\n",
            "\n",
            "Epoch 316\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0557 (batch time: 21.8078)\n",
            "[50000/50000] loss: 0.0367 (batch time: 20.6236)\n",
            "Time elapsed: 42.4562 seconds\n",
            "\n",
            "Epoch 317\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0434 (batch time: 21.6470)\n",
            "[50000/50000] loss: 0.0277 (batch time: 20.3288)\n",
            "Time elapsed: 42.0035 seconds\n",
            "\n",
            "Epoch 318\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0230 (batch time: 21.8415)\n",
            "[50000/50000] loss: 0.0414 (batch time: 21.3806)\n",
            "Time elapsed: 43.2645 seconds\n",
            "\n",
            "Epoch 319\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0367 (batch time: 20.7320)\n",
            "[50000/50000] loss: 0.0453 (batch time: 21.9096)\n",
            "Time elapsed: 42.6657 seconds\n",
            "\n",
            "Epoch 320\n",
            "-------------------------------\n",
            "[25000/50000] loss: 0.0289 (batch time: 20.5541)\n",
            "[50000/50000] loss: 0.0385 (batch time: 21.3475)\n",
            "Time elapsed: 41.9277 seconds\n",
            "\n",
            "Epoch 321\n",
            "-------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Set t really large if running for a specific number of times (endTime is in seconds)\n",
        "startTime = timer()\n",
        "endTime = 72000 # One hour = 3600\n",
        "lastEpochTime = startTime\n",
        "# Set e really high if running based on time\n",
        "e = 100000000000\n",
        "for epoch in range(e):\n",
        "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizerSGD)\n",
        "    #testing only on certain epochs\n",
        "    if epoch % 10 == 0:\n",
        "      test(testloader, model, loss_fn)\n",
        "\n",
        "    print(f\"Time elapsed: {round(timer() - lastEpochTime, 4)} seconds\\n\")\n",
        "    lastEpochTime = timer()\n",
        "    #PATH = f'./models/{dt_string}/epoch_{epoch + 1}.pth'\n",
        "    #torch.save(model.state_dict(), PATH)\n",
        "    if timer() - startTime >= endTime:\n",
        "        break\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z_B988a8d-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e36195c-f8df-4b7a-ca04-5e3194c902fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 89 %\n",
            "Accuracy for class: plane is 90.2 %\n",
            "Accuracy for class: car   is 96.1 %\n",
            "Accuracy for class: bird  is 89.4 %\n",
            "Accuracy for class: cat   is 79.2 %\n",
            "Accuracy for class: deer  is 88.5 %\n",
            "Accuracy for class: dog   is 82.8 %\n",
            "Accuracy for class: frog  is 91.1 %\n",
            "Accuracy for class: horse is 91.9 %\n",
            "Accuracy for class: ship  is 92.9 %\n",
            "Accuracy for class: truck is 93.3 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        # imgshow(make_grid(images)) # outputs all test images\n",
        "        output = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "class_probs = []\n",
        "class_label = []\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        output = model(images)\n",
        "\n",
        "        _, predictions = torch.max(output, 1)\n",
        "\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
        "\n",
        "        class_probs.append(class_probs_batch)\n",
        "        class_label.append(labels)\n",
        "\n",
        "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
        "test_label = torch.cat(class_label)\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_k_tyKpioU_"
      },
      "outputs": [],
      "source": [
        "# helper function\n",
        "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
        "    '''\n",
        "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
        "    precision-recall curve\n",
        "    '''\n",
        "    tensorboard_truth = test_label == class_index\n",
        "    tensorboard_probs = test_probs[:, class_index]\n",
        "\n",
        "    writer.add_pr_curve(classes[class_index],\n",
        "                        tensorboard_truth,\n",
        "                        tensorboard_probs,\n",
        "                        global_step=global_step)\n",
        "    writer.close()\n",
        "\n",
        "# plot all the pr curves\n",
        "for i in range(len(classes)):\n",
        "    add_pr_curve_tensorboard(i, test_probs, test_label)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}